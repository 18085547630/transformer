{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from src.dataset import OzeDataset\n",
    "from src.Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-2\n",
    "EPOCHS = 5\n",
    "TIME_CHUNK = True\n",
    "\n",
    "K = 672 # Time window length\n",
    "d_model = 48 # Lattent dim\n",
    "q = 8 # Query size\n",
    "v = 8 # Value size\n",
    "h = 4 # Number of heads\n",
    "N = 4 # Number of encoder and decoder to stack\n",
    "\n",
    "d_input = 37 # From dataset\n",
    "d_output = 8 # From dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(OzeDataset(\"dataset.npz\"),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        num_workers=NUM_WORKERS\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformer with Adam optimizer and MSE loss function\n",
    "net = Transformer(d_input, d_model, d_output, q, v, h, K, N, TIME_CHUNK)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare loss history\n",
    "hist_loss = np.zeros(EPOCHS)\n",
    "for idx_epoch in range(EPOCHS):\n",
    "    running_loss = 0\n",
    "    with tqdm(total=len(dataloader.dataset), desc=f\"[Epoch {idx_epoch+1:3d}/{EPOCHS}]\") as pbar:\n",
    "        for idx_batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Propagate input\n",
    "            netout = net(x)\n",
    "\n",
    "            # Comupte loss\n",
    "            loss = loss_function(netout, y)\n",
    "\n",
    "            # Backpropage loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': running_loss/(idx_batch+1)})\n",
    "            pbar.update(BATCH_SIZE)\n",
    "        \n",
    "    hist_loss[idx_epoch] = running_loss/len(dataloader)\n",
    "plt.plot(hist_loss, 'o-')\n",
    "print(f\"Loss: {float(hist_loss[-1]):5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select training example\n",
    "idx = np.random.randint(0, len(dataloader.dataset))\n",
    "x, y = dataloader.dataset[idx]\n",
    "\n",
    "# Run predictions\n",
    "with torch.no_grad():\n",
    "    x = torch.Tensor(x[np.newaxis, ...])\n",
    "    netout = net(x)\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "for idx_output_var in range(8):\n",
    "    # Select real temperature\n",
    "    y_true = y[:, idx_output_var]\n",
    "\n",
    "    y_pred = netout[0, :, idx_output_var]\n",
    "    y_pred = y_pred.numpy()\n",
    "\n",
    "    plt.subplot(8, 1, idx_output_var+1)\n",
    "    \n",
    "    plt.plot(y_true, label=\"Truth\")\n",
    "    plt.plot(y_pred, label=\"Prediction\")\n",
    "    plt.title(dataloader.dataset.labels[\"X\"][idx_output_var])\n",
    "plt.legend()\n",
    "plt.savefig(\"fig.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display encoding attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first encoding layer\n",
    "encoder = net.layers_encoding[0]\n",
    "\n",
    "# Get the first attention map\n",
    "attn_map = encoder.attention_map[0]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(attn_map)\n",
    "plt.savefig(\"attention_map.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
